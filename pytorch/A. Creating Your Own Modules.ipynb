{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial\n",
    "\n",
    "IFT6135 â€“ Representation Learning\n",
    "\n",
    "A Deep Learning Course, January 2019\n",
    "\n",
    "By Chin-Wei Huang \n",
    "\n",
    "(Adapted from Sandeep Subramanian's 2017 MILA tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your Own Modules\n",
    "\n",
    "### `torch.nn.module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``nn.Module`` is base class for all neural network modules.\n",
    "\n",
    "You should also write your modules as sub-class of ``nn.Module``, so that it can inherit the following attributes:\n",
    "\n",
    "* *Recursive structure*: you can wrap an instantiation of a Module class with another one, which stores the inner one as its parent\n",
    "\n",
    "* *Cudafiability*: you can easily cudafy the whole sequence of modules using `model.cuda()`\n",
    "\n",
    "* *Serializable*: you can save your trained model (checkpoint, early stopping ...) using ``torch.save``, ``torch.load``\n",
    "\n",
    "* *Parameters*: you can call model.parameters() to access all parameters at the same time. \n",
    "\n",
    "etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/docs/master/nn.html#linear-layers\n",
    "class Linear(nn.Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias. Default: True\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, in\\_features)`\n",
    "        - Output: :math:`(N, out\\_features)`\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape (out_features x in_features)\n",
    "        bias:   the learnable bias of the module of shape (out_features)\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = autograd.Variable(torch.randn(128, 20))\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.bias is None:\n",
    "            return self._backend.Linear()(input, self.weight)\n",
    "        else:\n",
    "            return self._backend.Linear()(input, self.weight, self.bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + str(self.in_features) + ' -> ' \\\n",
    "            + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.bias is None:\n",
    "            return torch.mm(input, self.weight) \n",
    "        else:\n",
    "            return torch.mm(input, self.weight) + self.bias\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randn(2, 3)).float()\n",
    "\n",
    "\n",
    "linear1 = nn.Linear(3,4)\n",
    "linear2 = MyLinear(3,4)\n",
    "\n",
    "# set the weight and bias of linear2 to be the same as linear1's\n",
    "linear2.weight.data = linear1.weight.data.transpose(1,0)\n",
    "linear2.bias.data = linear1.bias.data\n",
    "\n",
    "print(torch.eq(linear1(x), linear2(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet example\n",
    "\n",
    "* Resnet blocks let the gradient flow through the hidden unit more directly and at the same time increase expressiveness\n",
    "\n",
    "Res(x) = F(x, {W}) + x\n",
    "\n",
    "Res(x) = F(x, {W1}) + W2 x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, activation=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if in_features != out_features:\n",
    "            self.project_linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise(NotImplemented)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "exceptions must be old-style classes or derived from BaseException, not NotImplementedType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ca5969d8ab40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-ac002f2ae425>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNotImplemented\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: exceptions must be old-style classes or derived from BaseException, not NotImplementedType"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randn(2, 3)).float()\n",
    "\n",
    "\n",
    "res1 = nn.Linear(3,3)\n",
    "res2 = ResLinear(3,5)\n",
    "\n",
    "print(res1(x).size())\n",
    "print(res2(x).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting things altogether, Sequential, Parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, Linear=ResLinear):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.predict_ = nn.Sequential(\n",
    "            Linear(784, 328),\n",
    "            nn.ReLU(),\n",
    "            Linear(328, 328),\n",
    "            nn.ReLU(),\n",
    "            Linear(328, 10),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        return F.softmax(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return torch.max(self.predict_proba(x))[1]\n",
    "        # ``max'' returns (max_value, argmax)\n",
    "    \n",
    "    def loss(self, x, target):\n",
    "        proba = self.predict_(x)\n",
    "        return self.criterion(proba, target)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caveate:    ``CrossEntropyLoss``    versus    ``NLLLoss``\n",
    "\n",
    "\n",
    "* ``CrossEntropyLoss`` takes in *pre-softmax* as input\n",
    "\n",
    "* ``NLLLoss`` takes in *log-softmax* as input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9114)\n",
      "tensor(1.9114)\n"
     ]
    }
   ],
   "source": [
    "y = torch.Tensor(1,10).normal_()\n",
    "t = torch.from_numpy(np.random.choice(10, size=1))\n",
    "\n",
    "loss1 = nn.CrossEntropyLoss()\n",
    "loss2 = nn.NLLLoss()\n",
    "\n",
    "print(loss1(y, t))\n",
    "print(loss2(nn.LogSoftmax(dim=1)(y), t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3960)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randn(64, 784)).float()\n",
    "t = torch.from_numpy(np.random.choice(10, size=64))\n",
    "\n",
    "model = MyModel()\n",
    "print(model.loss(x, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Parameters (Manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4299)\n",
      "tensor(1.5486)\n",
      "tensor(1.0176)\n",
      "tensor(0.6854)\n",
      "tensor(0.4835)\n",
      "tensor(0.3032)\n",
      "tensor(0.2134)\n",
      "tensor(0.1655)\n",
      "tensor(0.1335)\n",
      "tensor(0.1107)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randn(64, 784)).float()\n",
    "t = torch.from_numpy(np.random.choice(10, size=64))\n",
    "model = MyModel()\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    loss = model.loss(x, t)\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        #param.data = param.data - lr*param.grad.data\n",
    "        param.data.sub_(param.grad.data*lr)\n",
    "        param.grad.data.zero_()\n",
    "        \n",
    "    print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Parameters (``torch.optim``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3423)\n",
      "tensor(1.6160)\n",
      "tensor(1.1222)\n",
      "tensor(0.7956)\n",
      "tensor(0.5599)\n",
      "tensor(0.4007)\n",
      "tensor(0.2985)\n",
      "tensor(0.2301)\n",
      "tensor(0.1816)\n",
      "tensor(0.1471)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randn(64, 784)).float()\n",
    "t = torch.from_numpy(np.random.choice(10, size=64))\n",
    "model = MyModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.0)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model.loss(x, t)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "        \n",
    "    print(loss)\n",
    "\n",
    "# what happens if you use momentum? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
